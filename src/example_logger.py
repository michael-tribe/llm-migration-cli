import json
from pathlib import Path

from aiofiles import open as aio_open

from src.dtos.example import Example
from src.logger import root_logger
from src.settings import settings


log = root_logger.getChild(__name__)


class ExampleLogger:
    file_to_input_hash_map: dict[str, set[str]] = {}

    def __init__(self, output_dir: Path | str | None = None) -> None:
        if isinstance(output_dir, str):
            output_dir = Path(output_dir)

        self.output_dir = output_dir or settings.paths.default_output_dir
        self.output_dir.mkdir(parents=True, exist_ok=True)

    def log(
        self,
        template: str,
        template_params: dict[str, str],
        output_text: str,
        llm_name: str,
        prompt_name: str | None = None,
        inference_params: dict | None = None,
        metadata: dict | None = None,
    ) -> None:
        """
        Log an example to disk.

        Args:
            template (str): The template used to generate the example.
            template_params (dict[str, str]): The parameters used to fill the template. If the input is not templated, pass an empty dict.
            output_text (str): The output text generated by the model.
            llm_name (str): The name of the model used to generate the example.
            prompt_name (str | None): The name of the prompt used to generate the example. If not available or provided, a name will be generated based on a hash of the template.
            inference_params (dict | None): Any model parameters used to generate the output text, like temperature, max_tokens, etc.
            metadata (dict | None): Additional metadata to log with the example.

        Returns:
            None
        """
        example = Example(  # type: ignore
            llm_name=llm_name,
            template=template,
            template_params=template_params,
            output_text=output_text,
            prompt_name=prompt_name,
            inference_params=inference_params,
            metadata=metadata,
        )
        output_path = self._determine_output_path(example)
        log.debug(f"Logging example to {output_path}")
        with output_path.open("a") as f:
            f.write(example.model_dump_json() + "\n")

    async def a_log(
        self,
        template: str,
        template_params: dict[str, str],
        output_text: str,
        llm_name: str,
        prompt_name: str | None = None,
        inference_params: dict | None = None,
        metadata: dict | None = None,
    ) -> None:
        """
        Log an example to disk asynchronously using aiofiles.

        Args:
            template (str): The template used to generate the example.
            template_params (dict[str, str]): The parameters used to fill the template. If the input is not templated, pass an empty dict.
            output_text (str): The output text generated by the model.
            llm_name (str): The name of the model used to generate the example.
            prompt_name (str | None): The name of the prompt used to generate the example. If not available or provided, a name will be generated based on a hash of the template.
            inference_params (dict | None): Any model parameters used to generate the output text, like temperature, max_tokens, etc.
            metadata (dict | None): Additional metadata to log with the example.

        Returns:
            None
        """
        example = Example(
            llm_name=llm_name,
            template=template,
            template_params=template_params,
            output_text=output_text,
            prompt_name=prompt_name,
            inference_params=inference_params,
            metadata=metadata,
        )
        if self._example_already_logged(example):
            log.debug(f"Example already logged (input hash: {example.input_hash}, llm_name: {llm_name})")
            return

        output_path = self._determine_output_path(example)
        self.file_to_input_hash_map[str(output_path)].add(example.input_hash + example.llm_name)  # type: ignore
        log.debug(f"Logging example to {output_path}")
        async with aio_open(output_path, "a") as f:
            await f.write(example.model_dump_json() + "\n")

    def _determine_output_path(self, example: Example) -> Path:
        filename = settings.paths.evaluation_examples_filename
        output_path = self.output_dir / example.prompt_name / filename  # type: ignore
        output_path.parent.mkdir(exist_ok=True, parents=True)
        return output_path

    def _example_already_logged(self, example: Example) -> bool:
        example_hash = example.input_hash + example.llm_name  # type: ignore
        output_path = self._determine_output_path(example)
        if str(output_path) not in self.file_to_input_hash_map:
            self._build_file_to_input_hash_map(output_path)

        if example_hash in self.file_to_input_hash_map[str(output_path)]:
            return True

        return False

    def _build_file_to_input_hash_map(self, output_path: Path) -> None:
        if not output_path.exists():
            self.file_to_input_hash_map[str(output_path)] = set()
            return

        with output_path.open() as f:
            examples = [Example(**json.loads(line)) for line in f]
            self.file_to_input_hash_map[str(output_path)] = {
                example.input_hash + example.llm_name for example in examples  # type: ignore
            }
